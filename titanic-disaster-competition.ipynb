{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Step 1 :\n\n- Import packages\n- Load dataset","metadata":{}},{"cell_type":"code","source":"# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\n\n# For data modeling\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# For metrics and helpful functions\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.tree import plot_tree\nfrom sklearn import metrics\n\n# Future Warning\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:18.891012Z","iopub.execute_input":"2025-01-16T15:42:18.891373Z","iopub.status.idle":"2025-01-16T15:42:21.030401Z","shell.execute_reply.started":"2025-01-16T15:42:18.891334Z","shell.execute_reply":"2025-01-16T15:42:21.029350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\n\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.031502Z","iopub.execute_input":"2025-01-16T15:42:21.031944Z","iopub.status.idle":"2025-01-16T15:42:21.061728Z","shell.execute_reply.started":"2025-01-16T15:42:21.031915Z","shell.execute_reply":"2025-01-16T15:42:21.060690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1) Exploratory Data Analysis (EDA)\n## a) Loading and initial exploration","metadata":{}},{"cell_type":"code","source":"# Head data\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-16T15:42:21.062685Z","iopub.execute_input":"2025-01-16T15:42:21.063001Z","iopub.status.idle":"2025-01-16T15:42:21.087741Z","shell.execute_reply.started":"2025-01-16T15:42:21.062977Z","shell.execute_reply":"2025-01-16T15:42:21.086826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Columns : \n- PassengerID : Passenger ID. It's the index.\n- Survived : 0 = No, 1 = Yes\n- Pclass : Ticket class. 1 = 1st, 2 = 2nd, 3 = 3rd\n- Name\n- Sex\n- Age\n- SibSp: Siblings/Spouses Aboard. Mistresses and fiancés were ignored.\n- Parch : Parents/Children Aboard. Some children travelled only with a nanny, therefore parch=0 for them.\n- Ticket : Ticket Number\n- Fare : Passenger Fare\n- Cabin : Cabin number\n- Embarked : Port of Embarkation. C = Cherbourg, Q = Queenstown, S = Southampton\n","metadata":{}},{"cell_type":"code","source":"#head test\ntest_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.088568Z","iopub.execute_input":"2025-01-16T15:42:21.088826Z","iopub.status.idle":"2025-01-16T15:42:21.101805Z","shell.execute_reply.started":"2025-01-16T15:42:21.088804Z","shell.execute_reply":"2025-01-16T15:42:21.100721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test_data shows the same columns, except survived, which is hidden. This will allow you to evaluate the model you've created.","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.103030Z","iopub.execute_input":"2025-01-16T15:42:21.103405Z","iopub.status.idle":"2025-01-16T15:42:21.139029Z","shell.execute_reply.started":"2025-01-16T15:42:21.103375Z","shell.execute_reply":"2025-01-16T15:42:21.137971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dtype of several columns will have to be reworked to be able to create a model, for example 'Sex', 'Embarked' ...","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.141999Z","iopub.execute_input":"2025-01-16T15:42:21.142269Z","iopub.status.idle":"2025-01-16T15:42:21.171111Z","shell.execute_reply.started":"2025-01-16T15:42:21.142244Z","shell.execute_reply":"2025-01-16T15:42:21.170103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are 891 ID on the dataset.\nThere must be missing data in the 'Age' dataset. The average age is 29.69.\nThe survival rate is 38%.","metadata":{}},{"cell_type":"code","source":"#check NaN value\nisnull_train = train_data.isnull().sum()\n\nisnull_test = test_data.isnull().sum()\n\nprint('Train :\\n',isnull_train,'\\n')\nprint('-'*40)\nprint('Test :\\n',isnull_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.173213Z","iopub.execute_input":"2025-01-16T15:42:21.173562Z","iopub.status.idle":"2025-01-16T15:42:21.184808Z","shell.execute_reply.started":"2025-01-16T15:42:21.173530Z","shell.execute_reply":"2025-01-16T15:42:21.183787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check Nan\ndef percentage_null(data, col) :\n    value = (data[col].isnull().sum() / len(data['PassengerId']))*100\n    return value\n    \ncolumns = ['Age', 'Cabin', 'Embarked']\n\nfor col in columns :\n    percentage = percentage_null(train_data, col)\n    print('The percentage of null values for',col, 'is :' ,percentage.round(2), '%' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.186103Z","iopub.execute_input":"2025-01-16T15:42:21.186424Z","iopub.status.idle":"2025-01-16T15:42:21.205244Z","shell.execute_reply.started":"2025-01-16T15:42:21.186400Z","shell.execute_reply":"2025-01-16T15:42:21.204063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusion :\n\n- The number of missing values in the “Cabin” column is important. Depending on the data exploration, I'll see if I delete this column, or if it can still give me important information with this question: Why are they missing?\n\n- The number of missing values for age is also important, but the correlation with survival rate should be significant. I think we should keep the column and replace the missing values.\n\n- The 'Embarked' column has only 2 missing values, which is insignificant.","metadata":{}},{"cell_type":"code","source":"#Duplicate\nduplicate_train = train_data.duplicated().sum()\nduplicate_test = test_data.duplicated().sum()\n\nprint('Duplicate Train :', duplicate_train)\nprint('Duplicate Test :', duplicate_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.206401Z","iopub.execute_input":"2025-01-16T15:42:21.206765Z","iopub.status.idle":"2025-01-16T15:42:21.225973Z","shell.execute_reply.started":"2025-01-16T15:42:21.206729Z","shell.execute_reply":"2025-01-16T15:42:21.224654Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## b) Data cleaning","metadata":{}},{"cell_type":"code","source":"# Cleaning col 'Age'\n## Use the average of the 3 preceding and following data to replace the age by the average.\n\ndef cleaning_age(data, col, window) :\n    data[col] = data[col].fillna(data[col].rolling(window=window, center=True, min_periods=1).mean())\n    return data\n    \ntrain_and_test = [train_data, test_data]\n\nfor data in train_and_test :\n    data = cleaning_age(data,'Age',7)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.227481Z","iopub.execute_input":"2025-01-16T15:42:21.227818Z","iopub.status.idle":"2025-01-16T15:42:21.240977Z","shell.execute_reply.started":"2025-01-16T15:42:21.227791Z","shell.execute_reply":"2025-01-16T15:42:21.239791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cleaning Embarked\ntrain_data['Embarked'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.242246Z","iopub.execute_input":"2025-01-16T15:42:21.242613Z","iopub.status.idle":"2025-01-16T15:42:21.261377Z","shell.execute_reply.started":"2025-01-16T15:42:21.242582Z","shell.execute_reply":"2025-01-16T15:42:21.260094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I will replace the few missing values with the most represented port.","metadata":{}},{"cell_type":"code","source":"#replace Nan By S in train_data[Embarked]\ntrain_data['Embarked'].fillna('S', inplace=True)\n\n#replace NaN by median in test_data[Fare]\ntest_data['Fare'] = pd.to_numeric(test_data['Fare'], errors='coerce')\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.262583Z","iopub.execute_input":"2025-01-16T15:42:21.262958Z","iopub.status.idle":"2025-01-16T15:42:21.275980Z","shell.execute_reply.started":"2025-01-16T15:42:21.262924Z","shell.execute_reply":"2025-01-16T15:42:21.274947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## c) Outliers","metadata":{}},{"cell_type":"code","source":"# Set the figure and axes\nfig, ax = plt.subplots(1, 2, figsize=(30,5))\n\nsns.boxplot(x=train_data['Age'], orient='h',ax=ax[0])\nax[0].set_title(\"Outliers of age\", fontsize='20')\n\nsns.boxplot(x=train_data['Fare'], orient='h', ax=ax[1])\nax[1].set_title(\"Outliers of fare\", fontsize='20')\n\nplt.tight_layout()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.277042Z","iopub.execute_input":"2025-01-16T15:42:21.277439Z","iopub.status.idle":"2025-01-16T15:42:21.798234Z","shell.execute_reply.started":"2025-01-16T15:42:21.277408Z","shell.execute_reply":"2025-01-16T15:42:21.797192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There don't seem to be any outliers in the 'Age' column. Indeed, in 1912, it was quite possible to reach 80.\nAs for 'Fare', which averages £32, prices of up to £500 seem astonishing.\nThat's why I'll try to determine the number of outliers per class, as the price is necessarily related to the class.","metadata":{}},{"cell_type":"code","source":"print('Max Fare train:', train_data['Fare'].max())\nprint('-'*40)\nprint('Max Fare test', test_data['Fare'].max())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.799232Z","iopub.execute_input":"2025-01-16T15:42:21.799560Z","iopub.status.idle":"2025-01-16T15:42:21.806545Z","shell.execute_reply.started":"2025-01-16T15:42:21.799533Z","shell.execute_reply":"2025-01-16T15:42:21.805511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determine the number of rows containing outliers\ndef outlier_fare_by_class(data, col, class_col):\n    outliers_by_class = {}\n        # Browse each class in 'Pclass' (1, 2, 3)\n    for cls in data[class_col].unique():\n        class_data = data[data[class_col] == cls]\n        \n        # Establish lower and upper quartiles\n        q1 = class_data[col].quantile(0.25)\n        q3 = class_data[col].quantile(0.75)\n\n        # Determine the interquartile range\n        iqr = q3 - q1\n\n        # Calculate the outlier threshold value\n        upper_bound = q3 + 1.5 * iqr\n\n        # Find outlier by class\n        outliers = class_data[(class_data[col] > upper_bound)]\n        outliers_by_class[cls] = outliers\n        \n        # Display outlier information by class\n        print(f\"Class {cls} - Upper Outliers Threshold: {upper_bound}\")\n        print(f\"Number of outliers in Class {cls}: {len(outliers)}\")\n    print('-'*40)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.807613Z","iopub.execute_input":"2025-01-16T15:42:21.807976Z","iopub.status.idle":"2025-01-16T15:42:21.825057Z","shell.execute_reply.started":"2025-01-16T15:42:21.807943Z","shell.execute_reply":"2025-01-16T15:42:21.823931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Outlier for Fare in train_data :\\n')\nprint(outlier_fare_by_class(train_data, 'Fare','Pclass'))\nprint('Outlier for Fare in test_data :\\n')\nprint(outlier_fare_by_class(test_data, 'Fare','Pclass'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.826082Z","iopub.execute_input":"2025-01-16T15:42:21.826385Z","iopub.status.idle":"2025-01-16T15:42:21.869401Z","shell.execute_reply.started":"2025-01-16T15:42:21.826361Z","shell.execute_reply":"2025-01-16T15:42:21.868415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Evaluate Parch and SibSp\nnames = ['Train', 'Test']\nfor data, name in zip(train_and_test, names):\n    print(f\"Max value of Parch in {name}:\", data['Parch'].max())\n    print(f\"Max value of SibSp in {name}:\", data['SibSp'].max())\n    print('-'*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.870550Z","iopub.execute_input":"2025-01-16T15:42:21.870972Z","iopub.status.idle":"2025-01-16T15:42:21.879967Z","shell.execute_reply.started":"2025-01-16T15:42:21.870935Z","shell.execute_reply":"2025-01-16T15:42:21.878966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Large families were common in those days, and several parents or children could travel together.\n\n\n#### Conclusion :\nCertain types of models are more sensitive to outliers than others. When I get to the stage of building your model, consider whether to remove outliers for Fare, based on the type of model I decide to use.","metadata":{}},{"cell_type":"markdown","source":"## d) Visualization and feature engineering","metadata":{}},{"cell_type":"markdown","source":"We will now study the different variables using various visualizations to try to understand the correlations between them.\nFor visualizations, I don't think it's necessary to do this for train_data and test_data. It was relevant to check and correct the 2 datasets, but the 2 seem consistent with each other.","metadata":{}},{"cell_type":"code","source":"# Variable survived\nfig, ax = plt.subplots(2, 2, figsize=(22,15))\n\n#Survived by sex\nsns.countplot(data=train_data, x='Sex', hue='Survived', ax=ax[0,0])\nax[0,0].set_title(\"Survivors by gender\", fontsize='20')\nax[0,0].set_xlabel(\"Gender\", fontsize=12)\nax[0,0].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\n\n# Survived by Class\nsns.countplot(data=train_data, x='Pclass', hue='Survived', ax=ax[0,1])\nax[0,1].set_title(\"Survivors by Class\", fontsize='20')\nax[0,1].set_xlabel(\"Class\", fontsize=12)\nax[0,1].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout(pad=4)\n\n#Survived by Age and Class\nsns.violinplot(data=train_data, x='Pclass', y='Age', hue = 'Survived', split = True, ax=ax[1,0])\nax[1,0].set_title(\"Age distribution by class and survival rate\", fontsize='20')\nax[1,0].legend(title=\"Survived\", labels=[\"No\", \"Yes\"], handles=[mpatches.Patch(color='#3174a1', label='No'), mpatches.Patch(color='#e1812b', label='Yes')])\nax[1,0].set_xlabel(\"Class\", fontsize=12)\n\n#Survived by Fare\nbins = [0, 10, 20, 30, 50, 100, 200, 600]  \nlabels = [\"0-10\", \"10-20\", \"20-30\", \"30-50\", \"50-100\", \"100-200\", \"200+\"]\ntrain_data['Price_range'] = pd.cut(train_data['Fare'], bins=bins, labels=labels,ordered=True)\nsns.countplot(data=train_data, x='Price_range', hue='Survived', ax=ax[1,1])\nax[1,1].set_title(\"Survivors per ticket price\", fontsize='20')\nax[1,1].set_xlabel(\"Ticket price\", fontsize=12)\nax[1,1].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\n#delete columns['Price_range']\ntrain_data = train_data.drop(columns= \"Price_range\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:21.880972Z","iopub.execute_input":"2025-01-16T15:42:21.881305Z","iopub.status.idle":"2025-01-16T15:42:23.215926Z","shell.execute_reply.started":"2025-01-16T15:42:21.881280Z","shell.execute_reply":"2025-01-16T15:42:23.214762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Logically, there are 3 determining factors that influence the survival rate:\n- Money (with class and ticket price)\n- Age\n- Gender\n\nThe “women and children first” doctrine seems to apply to the sinking of the Titanic. For children, the third graph shows a higher survival rate. However, only the 3rd class appears to have dead infants.\n\nThis makes me wonder about the difference in survival rates between men and women in different classes.\n\n","metadata":{}},{"cell_type":"code","source":"sns.catplot(data=train_data,x=\"Pclass\",hue=\"Sex\",col=\"Survived\",kind=\"count\",height=5, aspect = 1)\n\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Survival Rate by Gender and Class\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:23.216935Z","iopub.execute_input":"2025-01-16T15:42:23.217214Z","iopub.status.idle":"2025-01-16T15:42:23.890082Z","shell.execute_reply.started":"2025-01-16T15:42:23.217191Z","shell.execute_reply":"2025-01-16T15:42:23.889059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The same applies to gender: 3rd class women have perished more than women in other classes.\n“Women and children first” is not particularly true of the 3rd class.\n\n\nWe've highlighted the most obvious relationships.\nWe're now going to explore new features to find other correlations with survival.\nThe first, which I think is interesting and which we haven't yet explored, is family. Does being alone or in a family influence survival rates?\n","metadata":{}},{"cell_type":"code","source":"#create columns 'Family size' and 'Alone'\ndef family(data) :\n    data['FamilySize']= data['SibSp'] + data['Parch'] + 1\n    data['Alone'] = (data['FamilySize'] == 1).astype(int)\n    return data\n\ntrain_and_test = [train_data, test_data]\n\nfor data in train_and_test :\n    data = family(data)\n    \n\nfig, ax = plt.subplots(1, 2, figsize=(22,8))\n\n# Survived by Family Size\nsns.countplot(data=train_data, x='FamilySize', hue='Survived', ax=ax[0])\nax[0].set_title(\"Survivors by Family size\", fontsize='20')\nax[0].set_xlabel(\"Family size\", fontsize=12)\nax[0].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\n# Survived Alone\nsns.countplot(data=train_data, x='Alone', hue='Survived', ax=ax[1])\nax[1].set_title(\"Survivors solo traveler\", fontsize='20')\nax[1].set_xlabel(\"Alone\", fontsize=12)\nax[1].set_xticks(ticks=[0,1], labels=['In family', 'Alone'])\nax[1].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\n#calculate survival_rate\nsurvival_rate = train_data.groupby('Alone')['Survived'].mean()\n\nprint('Survival Rate :', survival_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:23.894572Z","iopub.execute_input":"2025-01-16T15:42:23.894915Z","iopub.status.idle":"2025-01-16T15:42:24.614755Z","shell.execute_reply.started":"2025-01-16T15:42:23.894887Z","shell.execute_reply":"2025-01-16T15:42:24.613758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It would seem that living with a family can significantly improve survival rates. \n\nIt may be easier for a family with children to board lifeboats than a single man.\n\nWe'll check this by separating the visualizations by gender.","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=train_data, x=\"Sex\", y=\"Survived\", hue=\"Alone\")\nplt.title('Survival rates by sex and status (alone or accompanied) aboard the Titanic')\nplt.xlabel('Gender')\nplt.ylabel('Survived (%)')\nplt.legend(title=\"Alone\",handles=[mpatches.Patch(color='#3174a1', label='No'), mpatches.Patch(color='#e1812b', label='Yes')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:24.616243Z","iopub.execute_input":"2025-01-16T15:42:24.616489Z","iopub.status.idle":"2025-01-16T15:42:25.031843Z","shell.execute_reply.started":"2025-01-16T15:42:24.616467Z","shell.execute_reply":"2025-01-16T15:42:25.030744Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unsurprisingly, single men have a lower survival rate. The opposite is true for women. Several hypotheses can be put forward:\n- Given that priority is given to women and children, not having children may have little impact on whether or not a woman gets into a lifeboat. It can even be a disadvantage: women on their own, not having to look after other people, could have boarded more quickly.\n- Accompanied women may come from lower social classes, where survival rates were lower.\n- Accompanied women (with children or partners) may have waited or hesitated to stay with their families.\n\n\nNow I'd like to extract the titles from the 'Name' column.\nI notice that the title seems to be made up in the same way: a capital letter followed by a period. It might be interesting to use this feature to extract it.","metadata":{}},{"cell_type":"code","source":"#create column [Title]\ndef title(data) :\n    data['Title'] = data['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    return data\n\nfor data in train_and_test:\n    data = title(data)\n\ntrain_data['Title'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.032977Z","iopub.execute_input":"2025-01-16T15:42:25.033370Z","iopub.status.idle":"2025-01-16T15:42:25.049382Z","shell.execute_reply.started":"2025-01-16T15:42:25.033331Z","shell.execute_reply":"2025-01-16T15:42:25.048294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Separate Title\ntitle_counts = train_data['Title'].value_counts()\ntitles_40plus = title_counts[title_counts >= 40].index\ntrain_data_40plus = train_data[train_data['Title'].isin(titles_40plus)]\n\ntitles_40minus = title_counts[title_counts < 40].index\ntrain_data_40minus = train_data[train_data['Title'].isin(titles_40minus)]\n\n# Create Vizualisation\nfig, ax = plt.subplots(1, 2, figsize=(22,8))\n\nsns.countplot(data=train_data_40plus, x='Title', hue='Survived', ax=ax[0])\nax[0].set_title(\"Survivors by title\", fontsize='20')\nax[0].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n\n\nsns.countplot(data=train_data_40minus, x='Title', hue='Survived', ax=ax[1])\nax[1].set_title(\"Survivors by title (Zoom for minus values)\", fontsize='20')\nax[1].legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation = 45)\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.050545Z","iopub.execute_input":"2025-01-16T15:42:25.050926Z","iopub.status.idle":"2025-01-16T15:42:25.558442Z","shell.execute_reply.started":"2025-01-16T15:42:25.050894Z","shell.execute_reply":"2025-01-16T15:42:25.557460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data by title corroborate previous analyses by gender and age : Miss and Master are unmarried women, often young, and teenagers.\nFinally, the titles provide additional information on very few passengers. Statuses such as Dr, Lady, Sir are so poorly represented that it's impossible to draw any conclusions.\n\n\nNow I'm going to look at the ticket variable. I'm going to see if each ticket is unique","metadata":{}},{"cell_type":"code","source":"train_data['Ticket'].duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.559478Z","iopub.execute_input":"2025-01-16T15:42:25.559794Z","iopub.status.idle":"2025-01-16T15:42:25.566511Z","shell.execute_reply.started":"2025-01-16T15:42:25.559769Z","shell.execute_reply":"2025-01-16T15:42:25.565451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To my surprise, there are many duplicate ticket numbers. Let's find out why.","metadata":{}},{"cell_type":"code","source":"\ndef groupsize(data) :\n    data['GroupSize'] = data.groupby('Ticket')['Ticket'].transform('count')\n    data[(data['GroupSize'] >1)]\n    return data\n\nfor data in train_and_test :\n    data = groupsize(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.567758Z","iopub.execute_input":"2025-01-16T15:42:25.568154Z","iopub.status.idle":"2025-01-16T15:42:25.585956Z","shell.execute_reply.started":"2025-01-16T15:42:25.568108Z","shell.execute_reply":"2025-01-16T15:42:25.584801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_data[(train_data['GroupSize'] > 1) & (train_data['Ticket'] == '113803')][['Name','Ticket']])\nprint('-' * 40)\nprint(train_data[(train_data['GroupSize'] > 1) & (train_data['Ticket'] == '349909')][['Name','Ticket']])\nprint('-' * 40)\nprint(train_data[(train_data['GroupSize'] > 1) & (train_data['Ticket'] == '347742')][['Name','Ticket']])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.586930Z","iopub.execute_input":"2025-01-16T15:42:25.587267Z","iopub.status.idle":"2025-01-16T15:42:25.602720Z","shell.execute_reply.started":"2025-01-16T15:42:25.587228Z","shell.execute_reply":"2025-01-16T15:42:25.601818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can therefore deduce that the 'Fare' column is skewed.\nTickets are not issued individually, but as a family.\nIn my opinion, it's more than appropriate to create a 'Fare' column for each individual.","metadata":{}},{"cell_type":"code","source":"# Create Fare per Person\ntrain_data['Fare_Per_Person'] = train_data['Fare'] / train_data['GroupSize']\ntest_data['Fare_Per_Person'] = test_data['Fare'] / test_data['GroupSize']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.603744Z","iopub.execute_input":"2025-01-16T15:42:25.604154Z","iopub.status.idle":"2025-01-16T15:42:25.618364Z","shell.execute_reply.started":"2025-01-16T15:42:25.604108Z","shell.execute_reply":"2025-01-16T15:42:25.617441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There's only one variable left to analyze: 'Cabin'.\nI'm aware that there are a large number of unknown values.\nSo I'm going to extract the first letter for each cabin entered, as from my research it seems that they correspond to the decks of the boat.","metadata":{}},{"cell_type":"code","source":"# Extract first letter and replace missing values with 'U' (Unknow).\ndef CabinLetter(data) :\n    data['CabinLetter'] = data['Cabin'].str[0]\n    data['CabinLetter'] = data['Cabin'].fillna('U').str[0]\n    return data\n\nfor data in train_and_test :\n    data = CabinLetter(data)\n    \n\nprint(train_data[['Cabin', 'CabinLetter']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.619306Z","iopub.execute_input":"2025-01-16T15:42:25.619676Z","iopub.status.idle":"2025-01-16T15:42:25.638384Z","shell.execute_reply.started":"2025-01-16T15:42:25.619640Z","shell.execute_reply":"2025-01-16T15:42:25.637332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Explore Cabin Letter\nsns.countplot(data=train_data, x='CabinLetter', hue='Survived')\nplt.xlabel('Boat Deck')\nplt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\nplt.title('Deck influence on survival', )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.639542Z","iopub.execute_input":"2025-01-16T15:42:25.639915Z","iopub.status.idle":"2025-01-16T15:42:25.985546Z","shell.execute_reply.started":"2025-01-16T15:42:25.639877Z","shell.execute_reply":"2025-01-16T15:42:25.984492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that the cabin letter has an influence on the survival rate. Column U (Unknow) has the lowest survival rate.\nBy hypothesis, we could deduce that the lowest-cost tickets don't have a registered cabin, only the most affluent do.","metadata":{}},{"cell_type":"code","source":"#Explore Cabin Letter\nsns.countplot(data=train_data, x='CabinLetter', hue='Pclass')\nplt.xlabel('Boat Deck')\nplt.legend(title=\"Class\")\nplt.title('Class by deck', )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:25.986651Z","iopub.execute_input":"2025-01-16T15:42:25.987004Z","iopub.status.idle":"2025-01-16T15:42:26.370769Z","shell.execute_reply.started":"2025-01-16T15:42:25.986979Z","shell.execute_reply":"2025-01-16T15:42:26.369776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This proves that the cabins that are not indicated are mainly in 3rd class.\nGiven the proportions of missing values, I fear that grouping the 3 classes in the same U variable may induce a bias, given the over-representation of the 3rd class.\nSo we're going to look at various ways of being more precise.\n\nMy first idea is to combine the tickets, which represent a group, and see if the decks are missing from some and present in others. My idea is that, if we're traveling in a group, it seems logical to me that we should at least travel on the same deck. I could then replace the missing values with the values of the members of the other group.","metadata":{}},{"cell_type":"code","source":"###### Train data\n\n# Check if tickets have both “U” and a valid letter\nticket_groups = train_data.groupby('Ticket').agg(\n    has_U=('CabinLetter', lambda x: (x == 'U').any()), \n    has_valid=('CabinLetter', lambda x: (x != 'U').any()) \n)\n\n# Identify tickets with inconsistencies\nticket_groups['inconsistency'] = ticket_groups['has_U'] & ticket_groups['has_valid']\n\n# Filter out inconsistent groups\ninconsistent_tickets = ticket_groups[ticket_groups['inconsistency']].reset_index()\n\n# Display results\nprint(\"Number of tickets with inconsistencies in train_data: \", len(inconsistent_tickets))\nprint(inconsistent_tickets)\n\nprint('-'*40)\n\n###### Test data\n\n# Check if tickets have both “U” and a valid letter\nticket_groups1 = test_data.groupby('Ticket').agg(\n    has_U=('CabinLetter', lambda x: (x == 'U').any()), \n    has_valid=('CabinLetter', lambda x: (x != 'U').any()) \n)\n\n# Identify tickets with inconsistencies\nticket_groups1['inconsistency'] = ticket_groups1['has_U'] & ticket_groups1['has_valid']\n\n# Filter out inconsistent groups\ninconsistent_tickets1 = ticket_groups1[ticket_groups1['inconsistency']].reset_index()\n\n# Display results\nprint(\"Number of tickets with inconsistencies in test_data: \", len(inconsistent_tickets1))\nprint(inconsistent_tickets1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:26.371994Z","iopub.execute_input":"2025-01-16T15:42:26.372375Z","iopub.status.idle":"2025-01-16T15:42:26.595658Z","shell.execute_reply.started":"2025-01-16T15:42:26.372339Z","shell.execute_reply":"2025-01-16T15:42:26.594525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter data with inconsistent tickets\ninconsistent_data = train_data[train_data['Ticket'].isin(inconsistent_tickets['Ticket'])]\ninconsistent_data_test = test_data[test_data['Ticket'].isin(inconsistent_tickets1['Ticket'])]\n\nfig, ax = plt.subplots(1, 2, figsize=(22,8))\n\n# View inconsistencies by ticket\nsns.countplot(data=inconsistent_data, x='Ticket', hue='CabinLetter', ax=ax[0])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation = 90)\nax[0].set_title('Cabin inconsistencies by ticket in train_data')\nax[0].set_xlabel(\"Ticket\", fontsize=12)\n\nsns.countplot(data=inconsistent_data_test, x='Ticket', hue='CabinLetter', ax=ax[1])\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation = 90)\nax[1].set_title('Cabin inconsistencies by ticket in test_data')\nax[1].set_xlabel(\"Ticket\", fontsize=12)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:26.596753Z","iopub.execute_input":"2025-01-16T15:42:26.597107Z","iopub.status.idle":"2025-01-16T15:42:27.456346Z","shell.execute_reply.started":"2025-01-16T15:42:26.597068Z","shell.execute_reply":"2025-01-16T15:42:27.455205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that I've visualized this, I can replace all the U values with the known values of the other group members.","metadata":{}},{"cell_type":"code","source":"# Create a function that replaces “U” with the known booth of the same ticket\ndef fill_cabin_from_group(group):\n    # Check whether a known booth is present in the group\n    known_cabin = group[group['CabinLetter'] != 'U']['CabinLetter'].unique()\n    if len(known_cabin) > 0:  \n        group['CabinLetter'] = group['CabinLetter'].replace('U', known_cabin[0])\n    return group\n\n# Apply function for each ticket group\ntrain_data = train_data.groupby('Ticket').apply(fill_cabin_from_group).reset_index(drop=True)\ntest_data = test_data.groupby('Ticket').apply(fill_cabin_from_group).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:27.457417Z","iopub.execute_input":"2025-01-16T15:42:27.457720Z","iopub.status.idle":"2025-01-16T15:42:28.597018Z","shell.execute_reply.started":"2025-01-16T15:42:27.457694Z","shell.execute_reply":"2025-01-16T15:42:28.596150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I don't see any other way of finding the deck with the other data. So I'm going to classify the rest of the U's into 3:\nU1 for Pclass1, U2 for Pclass2 and U3 for Pclass3.","metadata":{}},{"cell_type":"code","source":"# Function to classify the remaining “U ”s according to class\ndef classify_remaining_U(row):\n    if row['CabinLetter'] == 'U':\n        return f\"U{row['Pclass']}\"\n    return row['CabinLetter']\n\n# Apply function to CabinLetter column\ntrain_data['CabinLetter'] = train_data.apply(classify_remaining_U, axis=1)\ntest_data['CabinLetter'] = test_data.apply(classify_remaining_U, axis=1)\n# Check results\nprint(train_data['CabinLetter'].value_counts())\nprint(''*40)\nprint(test_data['CabinLetter'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.597980Z","iopub.execute_input":"2025-01-16T15:42:28.598260Z","iopub.status.idle":"2025-01-16T15:42:28.622658Z","shell.execute_reply.started":"2025-01-16T15:42:28.598237Z","shell.execute_reply":"2025-01-16T15:42:28.621628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It looks pretty good, but there's just one value I'm not sure about: T. There's no T bridge, so it's probably an error.","metadata":{}},{"cell_type":"code","source":"train_data[train_data['CabinLetter'] == 'T']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.623609Z","iopub.execute_input":"2025-01-16T15:42:28.624005Z","iopub.status.idle":"2025-01-16T15:42:28.651415Z","shell.execute_reply.started":"2025-01-16T15:42:28.623977Z","shell.execute_reply":"2025-01-16T15:42:28.650452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It's a class 1, so I'll change the value to U1.","metadata":{}},{"cell_type":"code","source":"train_data['CabinLetter'] = train_data['CabinLetter'].replace('T', 'U1')\n\nprint(train_data['CabinLetter'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.652501Z","iopub.execute_input":"2025-01-16T15:42:28.652908Z","iopub.status.idle":"2025-01-16T15:42:28.669038Z","shell.execute_reply.started":"2025-01-16T15:42:28.652833Z","shell.execute_reply":"2025-01-16T15:42:28.667921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now I'm going to clean the dataset again. Using a correlation map, I'm going to remove the least relevant columns following the creation of features. To do this, I'll also need to do some one-hot encoding for categorical data.","metadata":{}},{"cell_type":"markdown","source":"## e) Data cleaning following feature creation, column deletion and one-hot-encoding\n\nGroup the rarest titles into a 'Rare' category","metadata":{}},{"cell_type":"code","source":"#Rare 'Title' category\n\ndef Title_rare(data) :\n    data['Title'] = data['Title'].replace(['Lady', 'the Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    \ntrain_and_test = [train_data, test_data]\n\nfor data in train_and_test :\n    data = Title_rare(data)\n\ntrain_data['Title'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.669950Z","iopub.execute_input":"2025-01-16T15:42:28.670316Z","iopub.status.idle":"2025-01-16T15:42:28.696121Z","shell.execute_reply.started":"2025-01-16T15:42:28.670291Z","shell.execute_reply":"2025-01-16T15:42:28.695267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'm now going to purge the redundant columns.\nFor me, the columns to exclude are the following: \n- Name: Full name does not provide direct predictive information.\n- PassengerId: Simple identifier, useless for prediction.\n- Ticket: Contains unstructured information, difficult to use. I still used this data to extract FarePerPerson.\n- Cabin: Too granular and often incomplete. CabinLetter is a good replacement.\n- SibSp and Parch: Their information is already included in FamilySize and Alone.\n- 'Fare' has been replaced by 'FarePerPerson', which is much more precise.","metadata":{}},{"cell_type":"code","source":"#columns to drop\ncolumns_to_drop = ['Name', 'Fare', 'Ticket', 'Cabin', 'SibSp', 'Parch']\n\n# delete columns\ntrain_data = train_data.drop(columns=columns_to_drop)\ntest_data = test_data.drop(columns=columns_to_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.696952Z","iopub.execute_input":"2025-01-16T15:42:28.697218Z","iopub.status.idle":"2025-01-16T15:42:28.703367Z","shell.execute_reply.started":"2025-01-16T15:42:28.697194Z","shell.execute_reply":"2025-01-16T15:42:28.702344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nCheck that the 2 datasets are ok.","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.704305Z","iopub.execute_input":"2025-01-16T15:42:28.704582Z","iopub.status.idle":"2025-01-16T15:42:28.725300Z","shell.execute_reply.started":"2025-01-16T15:42:28.704559Z","shell.execute_reply":"2025-01-16T15:42:28.724337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.726324Z","iopub.execute_input":"2025-01-16T15:42:28.726578Z","iopub.status.idle":"2025-01-16T15:42:28.746174Z","shell.execute_reply.started":"2025-01-16T15:42:28.726556Z","shell.execute_reply":"2025-01-16T15:42:28.745230Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Everything looks good. I'll now proceed with the one-hot-encoding.","metadata":{}},{"cell_type":"code","source":"#columns to encode\ncolumns_to_encode = ['Sex', 'Embarked', 'Title', 'CabinLetter']\n\n#get dummies\ntrain_data = pd.get_dummies(train_data, columns=columns_to_encode, drop_first=True, dtype=int)\ntest_data = pd.get_dummies(test_data, columns=columns_to_encode, drop_first=True, dtype=int)\n\ntrain_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.747390Z","iopub.execute_input":"2025-01-16T15:42:28.747742Z","iopub.status.idle":"2025-01-16T15:42:28.781156Z","shell.execute_reply.started":"2025-01-16T15:42:28.747717Z","shell.execute_reply":"2025-01-16T15:42:28.780220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I'll check whether the train_data and test_data datasets have the same columns.","metadata":{}},{"cell_type":"code","source":"\nmissing_cols = set(train_data.columns) - set(test_data.columns)\nprint(\"Missing columns in test_data :\", missing_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.782199Z","iopub.execute_input":"2025-01-16T15:42:28.782551Z","iopub.status.idle":"2025-01-16T15:42:28.787651Z","shell.execute_reply.started":"2025-01-16T15:42:28.782519Z","shell.execute_reply":"2025-01-16T15:42:28.786830Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Everything's ok, the 2 data sets are identical, except for “Survived”, which is normal given that this is the data to be predicted.Now let's create a heatmap to find out the correlations.","metadata":{}},{"cell_type":"code","source":"train_data.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.788536Z","iopub.execute_input":"2025-01-16T15:42:28.788822Z","iopub.status.idle":"2025-01-16T15:42:28.805445Z","shell.execute_reply.started":"2025-01-16T15:42:28.788799Z","shell.execute_reply":"2025-01-16T15:42:28.804370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#columns for correlation \ncolumns_for_correlation = ['Survived', 'Pclass', 'Age', 'Fare_Per_Person', \n                            'FamilySize', 'Sex_male', 'Embarked_Q', \n                            'Embarked_S', 'Title_Mr', 'Title_Mrs']\n\nplt.figure(figsize=(16, 9))\nheatmap = sns.heatmap(train_data[columns_for_correlation].corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\nheatmap.set_title(\"Correlation Heatmap\", fontsize=20)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:42:28.806569Z","iopub.execute_input":"2025-01-16T15:42:28.806974Z","iopub.status.idle":"2025-01-16T15:42:29.405581Z","shell.execute_reply.started":"2025-01-16T15:42:28.806936Z","shell.execute_reply":"2025-01-16T15:42:29.404340Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The correlation map concludes with the same diagnosis: age, gender and class (including ticket price) seem to be most correlated with survival rate.","metadata":{}},{"cell_type":"markdown","source":"# 2) Model building\n## a) Choose which models to test\n\nI'm going to test 3 models: the Decision Tree, XGBoost and the Random Forest.\nI chose these 3 models because they are less sensitive to the outliers that we were able to detect on Fare, and which are always present on FarePerPerson.\n\nI separate the data data_train into train and test.\nThe data is unbalanced on the survival rate, so we will use the 'stratify' parameter.","metadata":{}},{"cell_type":"code","source":"# Isolate the target variable (y)\ny = train_data['Survived']\n\n# Isolate the features (X)\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:51:42.211339Z","iopub.execute_input":"2025-01-15T14:51:42.211703Z","iopub.status.idle":"2025-01-15T14:51:42.223420Z","shell.execute_reply.started":"2025-01-15T14:51:42.211671Z","shell.execute_reply":"2025-01-15T14:51:42.222213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## b) Decision Tree\n\nAt this stage we will begin the model optimisation process by constructing a decision tree model and set up cross-validated grid-search to exhuastively search for the best model parameters.\nAdditionally, we will also be setting our refit parameter to 'Accuracy',the metric used for the competition.","metadata":{}},{"cell_type":"code","source":"# Instantiate the decision tree classifer\ndt = DecisionTreeClassifier(random_state = 0)\n\n# Create a dictionary of hyperparameters to tune\ncv_params = {\"max_depth\": [2,4,6,8,10,15,20,30,40,50, None],\n             \"min_samples_leaf\": [2,3,4,5,6,7,8,9, 10, 15, 20, 50],\n             \"min_samples_split\": [2,3,4,5,6,7,8,9, 10, 15, 20, 50]}\n\n# Define a set of scoring metrics to capture\nscoring = {\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"}\n\n# Instantiate the GridSearchCV object\nclf_tree = GridSearchCV(dt,\n                        cv_params,\n                        scoring = scoring,\n                        cv = 5,\n                        refit = 'accuracy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:51:42.224630Z","iopub.execute_input":"2025-01-15T14:51:42.224981Z","iopub.status.idle":"2025-01-15T14:51:42.250948Z","shell.execute_reply.started":"2025-01-15T14:51:42.224954Z","shell.execute_reply":"2025-01-15T14:51:42.249896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will now fit the decision tree model to the training data.","metadata":{}},{"cell_type":"code","source":"clf_tree.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:51:42.252295Z","iopub.execute_input":"2025-01-15T14:51:42.252641Z","iopub.status.idle":"2025-01-15T14:53:45.214517Z","shell.execute_reply.started":"2025-01-15T14:51:42.252602Z","shell.execute_reply":"2025-01-15T14:53:45.213464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Examine the best Accuracy score achieved by the decision tree model on the training set.","metadata":{}},{"cell_type":"code","source":"clf_tree.best_score_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.215711Z","iopub.execute_input":"2025-01-15T14:53:45.216075Z","iopub.status.idle":"2025-01-15T14:53:45.222061Z","shell.execute_reply.started":"2025-01-15T14:53:45.216041Z","shell.execute_reply":"2025-01-15T14:53:45.221161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Examine the best combination of hyperparameters","metadata":{}},{"cell_type":"code","source":"# Determine the best combination of hyperparameters\nclf_tree.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.223087Z","iopub.execute_input":"2025-01-15T14:53:45.223455Z","iopub.status.idle":"2025-01-15T14:53:45.244076Z","shell.execute_reply.started":"2025-01-15T14:53:45.223422Z","shell.execute_reply":"2025-01-15T14:53:45.242933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we'll write a function called make_results() that will output all of the socres of our models. This function will accept three arguments.","metadata":{}},{"cell_type":"code","source":"def make_results(model_name:str, model_object, metric:str):\n    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {\"accuracy\": \"mean_test_accuracy\",\n                   \"precision\": \"mean_test_precision\",\n                   \"recall\": \"mean_test_recall\",\n                   \"f1\": \"mean_test_f1\",\n                   \"roc_auc\": \"mean_test_roc_auc\"}\n    \n    # Get all the results from the CV and put them in a df\n    cv_results = pd.DataFrame(model_object.cv_results_)\n    \n    # Isolate the row of the df with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n    \n    # Extract Accuracy, Precision, Recall, F1, and ROC_AUC socres from that row\n    accuracy = best_estimator_results.mean_test_accuracy\n    precision = best_estimator_results.mean_test_precision\n    recall = best_estimator_results.mean_test_recall\n    f1 = best_estimator_results.mean_test_f1\n    roc_auc = best_estimator_results.mean_test_roc_auc\n    \n    # Create a table of results\n    table = pd.DataFrame({\"Model\": [model_name],\n                          \"Accuracy\": [accuracy],\n                          \"Precision\": [precision],\n                          \"Recall\": [recall],\n                          \"F1\": [f1],\n                          \"ROC AUC\": [roc_auc]\n                         }\n                        )\n    \n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.245260Z","iopub.execute_input":"2025-01-15T14:53:45.245691Z","iopub.status.idle":"2025-01-15T14:53:45.267152Z","shell.execute_reply.started":"2025-01-15T14:53:45.245649Z","shell.execute_reply":"2025-01-15T14:53:45.265789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create results table from the output of our Decision Tree Classifier object using our training data\nresults = make_results(\"Decision Tree CV\", clf_tree, \"accuracy\")\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.268630Z","iopub.execute_input":"2025-01-15T14:53:45.269060Z","iopub.status.idle":"2025-01-15T14:53:45.307936Z","shell.execute_reply.started":"2025-01-15T14:53:45.269018Z","shell.execute_reply":"2025-01-15T14:53:45.306646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Overall analysis :\n- The model seems to perform well overall, with good accuracy and a good compromise between precision, recall and F1.\n- Recall can still be improved, as a model performing better in recall would find more survivors, which could be important in certain contexts (for example, if the aim is to predict as many survivors as possible).\n- The ROC AUC shows that your model has a good ability to separate classes, which is essential for binary classification problems.","metadata":{}},{"cell_type":"markdown","source":"## c) XGboost Model\n\nSimilar to the creation of our Decision Tree Model, to construct our XGBoost model, we will set up a cross-validated grid-search to iteratively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"## Instantiate the XGBoost classifier\nxgb = XGBClassifier(objective=\"binary:logistic\", random_state=0)\n\n# Create a dictionary of hyperparameters\ncv_params = {\"n_estimators\": [100, 150, 200, 300],\n    \"learning_rate\": [0.15, 0.2, 0.25],\n    \"max_depth\": [4, 5, 6],\n    \"subsample\": [0.7, 0.8, 0.9],\n    \"colsample_bytree\": [0.6, 0.7, 0.8],\n    \"gamma\": [0, 0.1, 0.5],\n    \"min_child_weight\": [4, 5, 6],\n    \"reg_alpha\": [0, 0.1, 0.2],\n    \"reg_lambda\": [1, 1.5, 2]}\n# Define a set of scoring metrics to capture\nscoring = {\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"}\n\n# Instantiate the GridSearchCV object\nxgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='accuracy')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.309172Z","iopub.execute_input":"2025-01-15T14:53:45.309643Z","iopub.status.idle":"2025-01-15T14:53:45.316988Z","shell.execute_reply.started":"2025-01-15T14:53:45.309608Z","shell.execute_reply":"2025-01-15T14:53:45.315685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will now fit the XGBoost model to the training data.","metadata":{}},{"cell_type":"code","source":"%%time\nxgb_cv = xgb_cv.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.318319Z","iopub.execute_input":"2025-01-15T14:53:45.318698Z","iopub.status.idle":"2025-01-15T14:53:45.796217Z","shell.execute_reply.started":"2025-01-15T14:53:45.318657Z","shell.execute_reply":"2025-01-15T14:53:45.794448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll examine the best accuracy score achieved by the XGBoost model on the training dataset .","metadata":{}},{"cell_type":"code","source":"xgb_cv.best_score_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.797480Z","iopub.execute_input":"2025-01-15T14:53:45.797915Z","iopub.status.idle":"2025-01-15T14:53:45.804264Z","shell.execute_reply.started":"2025-01-15T14:53:45.797872Z","shell.execute_reply":"2025-01-15T14:53:45.803203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Examine the best combination of hyperparameters","metadata":{}},{"cell_type":"code","source":"# Determine the best combination of hyperparameters\nxgb_cv.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.805176Z","iopub.execute_input":"2025-01-15T14:53:45.805514Z","iopub.status.idle":"2025-01-15T14:53:45.829047Z","shell.execute_reply.started":"2025-01-15T14:53:45.805460Z","shell.execute_reply":"2025-01-15T14:53:45.827607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create results table from the output of our XGBoost Classifier object using our training data\nxgb_results = make_results(\"XGBoost CV\", xgb_cv, \"accuracy\")\nxgb_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.830480Z","iopub.execute_input":"2025-01-15T14:53:45.830992Z","iopub.status.idle":"2025-01-15T14:53:45.861243Z","shell.execute_reply.started":"2025-01-15T14:53:45.830950Z","shell.execute_reply":"2025-01-15T14:53:45.860044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Overall analysis :\n- The model correctly classified around 84.12% of the examples, which is a good score, indicating that the model performs well in all predictions.\n- The model is relatively accurate when predicting the positive class (survival), with around 82% of its positive predictions being correct.\n- The model has a good survivor detection rate with recall.\n- The F1-Score, which balances precision and recall, indicates that the model has a good ability to predict both survivors and non-survivors.\n- The AUC of 88.07% shows that the model separates classes well, with a high ability to discriminate between survivors and non-survivors.","metadata":{}},{"cell_type":"markdown","source":"## d) Random Forest Model\n\nSimilar to the creation of our Decision Tree Model and our XGBoost Model, to construct our Random Forest Model we will set up a cross-validated grid-search to iteratively search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate the random forest classifier\nrf = RandomForestClassifier(random_state = 0)\n\n# Create a dictionary of hyperparameters to tune\ncv_params = {\"n_estimators\": [250, 300, 350],\n    \"max_depth\": [8, 10, 12],\n    \"max_features\": [0.6, 0.7, 0.8],\n    \"max_samples\": [0.9, 1.0],\n    \"min_samples_leaf\": [1, 2, 3],\n    \"min_samples_split\": [4, 5, 6]}\n\n# Define a set of scoring metrics to capture\nscoring = {\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"}\n\n# Instantiate the GridSearchCV object\nrf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='accuracy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.862256Z","iopub.execute_input":"2025-01-15T14:53:45.862570Z","iopub.status.idle":"2025-01-15T14:53:45.879883Z","shell.execute_reply.started":"2025-01-15T14:53:45.862539Z","shell.execute_reply":"2025-01-15T14:53:45.878622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will now fit the random forest model to the training data.","metadata":{}},{"cell_type":"code","source":"%%time\nrf_cv.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:45.881048Z","iopub.execute_input":"2025-01-15T14:53:45.881367Z","iopub.status.idle":"2025-01-15T14:53:50.244972Z","shell.execute_reply.started":"2025-01-15T14:53:45.881338Z","shell.execute_reply":"2025-01-15T14:53:50.243938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll examine the best accuracy score achieved by the random forest model on the training dataset .","metadata":{}},{"cell_type":"code","source":"rf_cv.best_score_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.246563Z","iopub.execute_input":"2025-01-15T14:53:50.246894Z","iopub.status.idle":"2025-01-15T14:53:50.252627Z","shell.execute_reply.started":"2025-01-15T14:53:50.246864Z","shell.execute_reply":"2025-01-15T14:53:50.251483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll examine the best combination of hyperparameters.","metadata":{}},{"cell_type":"code","source":"# Determine the best combination of hyperparameters\nrf_cv.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.253674Z","iopub.execute_input":"2025-01-15T14:53:50.253985Z","iopub.status.idle":"2025-01-15T14:53:50.275133Z","shell.execute_reply.started":"2025-01-15T14:53:50.253958Z","shell.execute_reply":"2025-01-15T14:53:50.273938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## e) Choose the best model\n\nWe will combine the evaluation scores on the training set for both the decision tree, XGBoost and random forest models.","metadata":{}},{"cell_type":"code","source":"# Get scores on the training data for the random forest model and add this to the existing results table\nrf_results = make_results(\"Random Forest CV\", rf_cv, \"accuracy\")\ncombined_results = pd.concat([results, rf_results,xgb_results ], axis=0)\ncombined_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.276241Z","iopub.execute_input":"2025-01-15T14:53:50.276624Z","iopub.status.idle":"2025-01-15T14:53:50.306927Z","shell.execute_reply.started":"2025-01-15T14:53:50.276594Z","shell.execute_reply":"2025-01-15T14:53:50.305850Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We chose XGBoost for this project because all the scores were higher or equal.\nWhat's more, we're looking for the best accuracy, and XGBoost has the best result. \nSo it seems obvious to me to choose this model.\n\nNow that we have chosen our model (XGBoost), we can evaluate this model on the test set. First we will use our model to predict on the test data and assign the results to a variable called rf_preds.","metadata":{}},{"cell_type":"code","source":"# Get scores on test data\nxgb_preds = xgb_cv.best_estimator_.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.308180Z","iopub.execute_input":"2025-01-15T14:53:50.308505Z","iopub.status.idle":"2025-01-15T14:53:50.335013Z","shell.execute_reply.started":"2025-01-15T14:53:50.308453Z","shell.execute_reply":"2025-01-15T14:53:50.333912Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the following get_test_scores() function to obtain the scores of our model on the test data.","metadata":{}},{"cell_type":"code","source":"def get_test_scores(model_name:str, preds, y_test_data):    \n    # Extract Accuracy, Precision, Recall, F1, and ROC AUC scores\n    accuracy = metrics.accuracy_score(y_test_data, preds)\n    precision = metrics.precision_score(y_test_data, preds)\n    recall = metrics.recall_score(y_test_data, preds)\n    f1 = metrics.f1_score(y_test_data, preds)\n    roc_auc = metrics.roc_auc_score(y_test_data, preds)\n    \n    # Create a table of results\n    table = pd.DataFrame({\"Model\": [model_name],\n                          \"Accuracy\": [accuracy],\n                          \"Precision\": [precision],\n                          \"Recall\": [recall],\n                          \"F1\": [f1],\n                          \"ROC AUC\": [roc_auc]\n                         }\n                        )\n    \n    return table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.335754Z","iopub.execute_input":"2025-01-15T14:53:50.336059Z","iopub.status.idle":"2025-01-15T14:53:50.359265Z","shell.execute_reply.started":"2025-01-15T14:53:50.336034Z","shell.execute_reply":"2025-01-15T14:53:50.357895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We'll now use the get_test_scores() function to generate the scores on the test data and assign the results to xgb_test_scores. We will concat these results to our combined table for comparison.","metadata":{}},{"cell_type":"code","source":"# Get scores on test data\nxgb_test_scores = get_test_scores(\"XGB Test\", xgb_preds, y_test)\ncombined_results = pd.concat([combined_results, xgb_test_scores], axis=0)\ncombined_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.360477Z","iopub.execute_input":"2025-01-15T14:53:50.361023Z","iopub.status.idle":"2025-01-15T14:53:50.399329Z","shell.execute_reply.started":"2025-01-15T14:53:50.360979Z","shell.execute_reply":"2025-01-15T14:53:50.398345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compared with cross-validation (XGBoost CV), we see a slight drop in performance on the test set, notably a reduction in Accuracy and ROC AUC. This indicates that XGBoost may present a slight over-fit to the training data, which is not uncommon with powerful models like this one.\n\n","metadata":{}},{"cell_type":"markdown","source":"## f) Feature importance\n\nBy way of illustration, I'm now going to look at the features importances of the decision tree and random forest models.\n\n#### Decision Tree","metadata":{}},{"cell_type":"code","source":"#plot the decision tree\nplt.figure(figsize=(70,24))\nplot_tree(clf_tree.best_estimator_, max_depth=6, fontsize=10, feature_names=X.columns,\n          class_names={0:\"Dead\", 1:\"Survived\"}, filled=True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:50.400349Z","iopub.execute_input":"2025-01-15T14:53:50.400758Z","iopub.status.idle":"2025-01-15T14:53:53.585539Z","shell.execute_reply.started":"2025-01-15T14:53:50.400717Z","shell.execute_reply":"2025-01-15T14:53:53.584349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the important features from the featured engineered decision tree model and save into a dataframe\nimportances = pd.DataFrame(clf_tree.best_estimator_.feature_importances_,\n                           columns=['gini_importance'],\n                           index=X.columns)\n\n# Sort the 'importances' dataframe in descending order\nimportances = importances.sort_values(by='gini_importance', ascending=False)\n\n# Only extract the features with importances > 0\nimportances = importances[importances['gini_importance'] > 0]\nimportances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:53.586739Z","iopub.execute_input":"2025-01-15T14:53:53.587221Z","iopub.status.idle":"2025-01-15T14:53:53.603393Z","shell.execute_reply.started":"2025-01-15T14:53:53.587176Z","shell.execute_reply":"2025-01-15T14:53:53.601748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a barplot to visualise the decision tree feature importances\nsns.barplot(data=importances, x='gini_importance', y=importances.index, orient='h')\nplt.title(\"Decision Tree: Feature Importances for Titanic Disaster\", fontsize=14)\nplt.ylabel(\"Feature\")\nplt.xlabel(\"Importance\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:53.604938Z","iopub.execute_input":"2025-01-15T14:53:53.605292Z","iopub.status.idle":"2025-01-15T14:53:53.972363Z","shell.execute_reply.started":"2025-01-15T14:53:53.605262Z","shell.execute_reply":"2025-01-15T14:53:53.971097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusion from the decision tree: \nThe most important feature is mainly gender, as we suspected.\nNext comes social class with Fare Per person; and Age.\nGroup size is more important than family size.","metadata":{}},{"cell_type":"markdown","source":"#### XGBoost\n\nWe'll now repeat the same process as previously undertaken to determine the feature importances for the random forest model","metadata":{}},{"cell_type":"code","source":"# Calculate the important features from the featured engineered random forest model and save to a new dataframe\nxgb_importances = pd.DataFrame(xgb_cv.best_estimator_.feature_importances_,\n                              columns=['gini_importance'],\n                              index=X.columns)\n\n# Sort the 'rf_importances' dataframe in descending order\nxgb_importances = xgb_importances.sort_values(by='gini_importance', ascending=False)\n\n# Only extract the features with importances > 0\nxgb_importances = xgb_importances[xgb_importances['gini_importance'] > 0]\nxgb_importances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:53.973352Z","iopub.execute_input":"2025-01-15T14:53:53.973682Z","iopub.status.idle":"2025-01-15T14:53:53.986190Z","shell.execute_reply.started":"2025-01-15T14:53:53.973654Z","shell.execute_reply":"2025-01-15T14:53:53.985065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.barplot(data=xgb_importances, x='gini_importance', y=xgb_importances.index, orient='h')\nplt.title(\"XGBoost : Feature Importances for Titanic disaster\", fontsize=14)\nplt.ylabel(\"Feature\")\nplt.xlabel(\"Importance\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:53.987227Z","iopub.execute_input":"2025-01-15T14:53:53.987538Z","iopub.status.idle":"2025-01-15T14:53:54.351731Z","shell.execute_reply.started":"2025-01-15T14:53:53.987513Z","shell.execute_reply":"2025-01-15T14:53:54.350373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusion for XGBoost model : \n\nAs with the Decision Tree, gender is really decisive.\nHere, Cabin Letter U3, which represents class 3, is of great importance. And the fare per person seems to be less important than with the Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"#### Random Forest\n\nWe'll now repeat the same process as previously undertaken to determine the feature importances for the random forest model.","metadata":{}},{"cell_type":"code","source":"# Calculate the important features from the featured engineered random forest model and save to a new dataframe\nrf_importances = pd.DataFrame(rf_cv.best_estimator_.feature_importances_,\n                              columns=['gini_importance'],\n                              index=X.columns)\n\n# Sort the 'rf_importances' dataframe in descending order\nrf_importances = rf_importances.sort_values(by='gini_importance', ascending=False)\n\n# Only extract the features with importances > 0\nrf_importances = rf_importances[rf_importances['gini_importance'] > 0]\nrf_importances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:54.353035Z","iopub.execute_input":"2025-01-15T14:53:54.353450Z","iopub.status.idle":"2025-01-15T14:53:54.390013Z","shell.execute_reply.started":"2025-01-15T14:53:54.353410Z","shell.execute_reply":"2025-01-15T14:53:54.388851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a barplot to visualise the random forest feature importances\nplt.figure(figsize=(12,7))\nsns.barplot(data=rf_importances, x='gini_importance', y=rf_importances.index, orient='h')\nplt.title(\"Random Forest: Feature Importances for Titanic Disaster\", fontsize=14)\nplt.ylabel(\"Feature\")\nplt.xlabel(\"Importance\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:54.391195Z","iopub.execute_input":"2025-01-15T14:53:54.391622Z","iopub.status.idle":"2025-01-15T14:53:54.930533Z","shell.execute_reply.started":"2025-01-15T14:53:54.391580Z","shell.execute_reply":"2025-01-15T14:53:54.929131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusion for Random Forest :\n\nThe Random Forest model strongly resembles the Decision Tree, where gender and Fare per person seem to be the determining factors.\n","metadata":{}},{"cell_type":"markdown","source":"# 3) Submission and Conclusion\n## a) Generate file for submission\n\nI prepare the test data by deleting PassengerID.","metadata":{}},{"cell_type":"code","source":"# Save PassengerID for submission\ntest_ids = test_data[\"PassengerId\"]\n\n#Delete for X_test\nX_test = test_data.drop(columns=[\"PassengerId\"])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:54.931532Z","iopub.execute_input":"2025-01-15T14:53:54.931845Z","iopub.status.idle":"2025-01-15T14:53:54.937945Z","shell.execute_reply.started":"2025-01-15T14:53:54.931816Z","shell.execute_reply":"2025-01-15T14:53:54.936645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test dataset predictions\npredictions = xgb_cv.best_estimator_.predict(X_test)\n\n#Creating a submission file\nsubmission = pd.DataFrame({\"PassengerId\": test_ids, \"Survived\": predictions})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created : submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:53:54.939429Z","iopub.execute_input":"2025-01-15T14:53:54.939827Z","iopub.status.idle":"2025-01-15T14:53:54.970243Z","shell.execute_reply.started":"2025-01-15T14:53:54.939787Z","shell.execute_reply":"2025-01-15T14:53:54.969250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## b) Conclusion\n\nWhat we can learn from this study is that the determining factors were mainly gender, social class and age.\nHowever, a closer look at the data reveals the complexity involved in predicting survival on a shipwreck such as this.\nOther details, which may seem insignificant (such as family size) can ultimately be decisive.\n\nI'd like to thank all those who have read this. This is my first Kaggle/ contest.\nDon't hesitate to vote positively if you liked it.","metadata":{}}]}